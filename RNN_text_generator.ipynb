{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14771,
     "status": "ok",
     "timestamp": 1527696205463,
     "user": {
      "displayName": "Andres Carvallo",
      "photoUrl": "//lh4.googleusercontent.com/-6BjOFE1P4Mk/AAAAAAAAAAI/AAAAAAAAAqI/qXhmQWXy3Y0/s50-c-k-no/photo.jpg",
      "userId": "103607418722343842699"
     },
     "user_tz": 240
    },
    "id": "V5VFVHk-97DB",
    "outputId": "8aebcf12-bd87-4639-ec66-cf54d4a2fdbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from keras import backend as K\n",
    "import scipy.io as sio\n",
    "from keras import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout,Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Convolution2D, BatchNormalization, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "import pickle\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import sys \n",
    "import re\n",
    "import heapq\n",
    "import random\n",
    "from IPython.display import Image\n",
    "\n",
    "# chequeamos que funcione GPU \n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dym-6ERO-Gu8"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/nlyqf5uulx6sr8d/ArchivosParte1.zip\n",
    "!unzip ArchivosParte1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "heuT0S3u-MKZ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('international-airline-passengers.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rKJMfzDH_qEK"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DpNKSs1nAVWH"
   },
   "outputs": [],
   "source": [
    "modelRNN = Sequential()\n",
    "modelRNN.add(SimpleRNN(5,input_dim=1,input_length=4,return_sequences=False))\n",
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cA6_PRw9Bea1"
   },
   "source": [
    "Total de parametros: 35  <br>\n",
    "Correspondiente a:<br>\n",
    "\n",
    "Wxh = 5 x 1 = 5 params   (entrada) <br>\n",
    "Whh = 5 x 5 = 25 params(hidden layer) <br> \n",
    "Why = 5 x 1 = 5 params (salida) <br>\n",
    "\n",
    "Total parametros = 35 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vBiHdY6nBB-m"
   },
   "outputs": [],
   "source": [
    "# con 4 dimensiones \n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(SimpleRNN(4,input_dim=1,input_length=4,return_sequences=False))\n",
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDf78frkCpRn"
   },
   "source": [
    "Ahora si modificamos la dimension a 4, el total de parametros baja a 24 explicado por: <br>\n",
    "\n",
    "Wxh = 4 x 1   (entrada)<br>\n",
    "Whh = 4 x 4 = 16 (hidden layer)<br> \n",
    "Why = 4 x 1 (salida) <br>\n",
    "\n",
    "Total parametros = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0NJMc40FQyg"
   },
   "source": [
    "Funciones Auxiliares Airline prediction (AirlinePrediction.py y AirlineUtils.py)  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xahlao6fFhvg"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "\n",
    "def readAirlineData(history):\n",
    "    # load the dataset\n",
    "    dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = 96 #Enero 1949 a Diciembre 1956\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    # reshape into X=t and Y=t+1\n",
    "    trainX, trainY = create_dataset(train, history)\n",
    "    testX, testY = create_dataset(test, history)\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1],1))\n",
    "    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1],1))\n",
    "    return trainX, trainY, testX, testY, scaler, dataset\n",
    "\n",
    "\n",
    "def displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history):\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    \n",
    "    #return trainScore, testScore\n",
    "    # shift train predictions for plotting\n",
    "    trainPredictPlot = numpy.empty_like(dataset)\n",
    "    trainPredictPlot[:, :] = numpy.nan\n",
    "    trainPredictPlot[history:len(trainPredict)+history, :] = trainPredict\n",
    "\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = numpy.empty_like(dataset)\n",
    "    testPredictPlot[:, :] = numpy.nan\n",
    "    testPredictPlot[len(trainPredict)+(history*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "    # plot baseline and predictions\n",
    "    plt.plot(scaler.inverse_transform(dataset))\n",
    "    plt.plot(trainPredictPlot)\n",
    "    plt.plot(testPredictPlot)\n",
    "    plt.show()\n",
    "    \n",
    "    return (trainScore, testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xv-8l82uCiwR"
   },
   "outputs": [],
   "source": [
    "#set sequence length\n",
    "history=4 \n",
    "\n",
    "#read training and test sets\n",
    "trainX, trainY, testX, testY, scaler, dataset=readAirlineData(history)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(SimpleRNN(5,input_dim=1,input_length=history, return_sequences=False))\n",
    "modelRNN.add(Dense(1))\n",
    "\n",
    "RMSE_results = []\n",
    "#Train model\n",
    "# ejecutamos 5 veces \n",
    "for i in range(5):\n",
    "\n",
    "  modelRNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  modelRNN.fit(trainX, trainY, nb_epoch=150, batch_size=5, verbose=2)\n",
    "\n",
    "  # Make predictions\n",
    "  trainPredict = modelRNN.predict(trainX)\n",
    "  testPredict = modelRNN.predict(testX)\n",
    "\n",
    "  #Display results\n",
    "  RMSE_results.append(displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history))\n",
    "  \n",
    "  # reseteamos los pesos de la red \n",
    "  modelRNN = Sequential()\n",
    "  modelRNN.add(SimpleRNN(5,input_dim=1,input_length=history, return_sequences=False))\n",
    "  modelRNN.add(Dense(1))\n",
    "\n",
    "RMSE_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGPLFNshfXLZ"
   },
   "source": [
    "En 5 iteraciones cambian los resultados de RMSE en set de train y testing:  <br>\n",
    "Train RMSE | Test RMSE <br>\n",
    "(28.601777834802732, 87.99367348269288), <br>\n",
    " (21.060396047062273, 65.64185217447363), <br>\n",
    " (20.541622444370542, 64.23740290571874), <br>\n",
    " (20.304950774496767, 68.34587708863847), <br>\n",
    " (20.49618758870263, 72.14143507462187) <br> \n",
    " \n",
    " Esto se puede deber a que los pesos iniciales son aleatorios y esto puede hacer que cambien los resultados de la prediccion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXkOiC5nbgH7"
   },
   "source": [
    "### Cambiando el largo de la secuencia de entrada (history), como afecta precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WgjAlgZDFVhA"
   },
   "outputs": [],
   "source": [
    "#set sequence length (cambiar y ver como afecta precision)\n",
    "input_length=4\n",
    "\n",
    "#read training and test sets\n",
    "trainX, trainY, testX, testY, scaler, dataset=readAirlineData(history)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(SimpleRNN(5,input_dim=1,input_length=input_length, return_sequences=False))\n",
    "modelRNN.add(Dense(1))\n",
    "\n",
    "#Train model\n",
    "modelRNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "modelRNN.fit(trainX, trainY, nb_epoch=150, batch_size=5, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = modelRNN.predict(trainX)\n",
    "testPredict = modelRNN.predict(testX)\n",
    "\n",
    "#Display results\n",
    "displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nZ55nIecucm"
   },
   "source": [
    "Length = 4 , RMSE = (22.13, 47.06)         <br>\n",
    "Length = 8 , RMSE =  (23.69, 61.11)       <br>\n",
    "Length = 12 , RMSE = (34.36, 111.65)      <br>\n",
    "\n",
    "Al aumentar el largo de la entrada tiene peor precision, mayor RMSE.\n",
    "\n",
    "Esto se puede deber a que al aumentar el largo de la entrada tiene menos datos de entrenamiento para mejorar la precision del modelo. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "L7VgS86Mbten"
   },
   "outputs": [],
   "source": [
    "#set sequence length (cambiar y ver como afecta precision)\n",
    "history=4\n",
    "\n",
    "#read training and test sets\n",
    "trainX, trainY, testX, testY, scaler, dataset=readAirlineData(history)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(SimpleRNN(5,input_dim=1,input_length=history, return_sequences=False))\n",
    "\n",
    "# agreamos nueva capa densa 100 unidades  \n",
    "modelRNN.add(Dense(100))\n",
    "\n",
    "# capa de salida dimension = 1 \n",
    "modelRNN.add(Dense(1))\n",
    "\n",
    "#Train model\n",
    "modelRNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "modelRNN.fit(trainX, trainY, nb_epoch=150, batch_size=5, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = modelRNN.predict(trainX)\n",
    "testPredict = modelRNN.predict(testX)\n",
    "\n",
    "#Display results\n",
    "displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I5c8r8AKgGw6"
   },
   "outputs": [],
   "source": [
    "#set sequence length (cambiar y ver como afecta precision)\n",
    "history=4\n",
    "\n",
    "#read training and test sets\n",
    "trainX, trainY, testX, testY, scaler, dataset=readAirlineData(history)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "modelRNN = Sequential()\n",
    "modelRNN.add(SimpleRNN(5,input_dim=1,input_length=history, return_sequences=False))\n",
    "\n",
    "# agregamos capa DROPOUT \n",
    "modelRNN.add(Dropout(0.5))\n",
    "\n",
    "# capa de salida \n",
    "modelRNN.add(Dense(1))\n",
    "\n",
    "#Train model\n",
    "modelRNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "modelRNN.fit(trainX, trainY, nb_epoch=150, batch_size=5, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = modelRNN.predict(trainX)\n",
    "testPredict = modelRNN.predict(testX)\n",
    "\n",
    "#Display results\n",
    "displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRiS4UsSlRPJ"
   },
   "source": [
    "Al agregar DROP OUT el RMSE aumenta considerablemente, sobre todo en set de testing... es decir empeora la precision del modelo al agregar DROPOUT. En el caso anterior el RMSE de test fue de 47 y ahora aumento a 94. \n",
    "Esto es porque el modelo queda con menos neuronas para modificar parametros y entrenar el modelo. \n",
    "Ademas como son tan pocas capas pierde muchos parametros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eY49X6VllJXP"
   },
   "outputs": [],
   "source": [
    "#set sequence length (cambiar y ver como afecta precision)\n",
    "history=4\n",
    "\n",
    "#read training and test sets\n",
    "trainX, trainY, testX, testY, scaler, dataset=readAirlineData(history)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "modelRNN = Sequential()\n",
    "\n",
    "# Aqui cambiamos por LSTM \n",
    "modelRNN.add(LSTM(5,input_dim=1,input_length=history, return_sequences=False))\n",
    "\n",
    "# capa de salida \n",
    "modelRNN.add(Dense(1))\n",
    "\n",
    "#Train model\n",
    "modelRNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "modelRNN.fit(trainX, trainY, nb_epoch=150, batch_size=5, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = modelRNN.predict(trainX)\n",
    "testPredict = modelRNN.predict(testX)\n",
    "\n",
    "#Display results\n",
    "displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJrlM-g9nG7y"
   },
   "source": [
    "**En el caso anterior el RMSE de test fue de 47 y ahora AUMENTO a 59.68** deberia mejorar pero como son tan pocos datos no alcanza a mejorar su prediccion. Lo que hace LSTM es solucionar el problema de que los weights se hagan muy pequeños o muy grandes al aplicar el gradiente. Pero en este caso como son tan pocas capas y datos este problema no alcanza a suceder. \n",
    "\n",
    "**En LSTM se usaron 146 parametros comparados con la SimpleRNN que uso tan solo 41 parametros. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oy6TPNl0xngp"
   },
   "outputs": [],
   "source": [
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wv11pBtQLSHT"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "svI1ZOSDLQ1X"
   },
   "outputs": [],
   "source": [
    "#set sequence length (cambiar y ver como afecta precision)\n",
    "history=4\n",
    "\n",
    "#read training and test sets\n",
    "trainX, trainY, testX, testY, scaler, dataset=readAirlineData(history)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "modelRNN = Sequential()\n",
    "\n",
    "# Aqui cambiamos por GRU \n",
    "modelRNN.add(GRU(5,input_dim=1,input_length=history, return_sequences=False))\n",
    "\n",
    "# Capa de salida \n",
    "modelRNN.add(Dense(1))\n",
    "\n",
    "#Train model\n",
    "modelRNN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "modelRNN.fit(trainX, trainY, nb_epoch=150, batch_size=5, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "trainPredict = modelRNN.predict(trainX)\n",
    "testPredict = modelRNN.predict(testX)\n",
    "\n",
    "#Display results\n",
    "displayResult(dataset, trainPredict, trainY, testPredict, testY, scaler, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY2t-j3oLprO"
   },
   "source": [
    "**En el caso anterior el RMSE de test fue de 47 y ahora AUMENTO a 56.66** deberia mejorar pero como son tan pocos datos no alcanza a mejorar su prediccion. <br> \n",
    "\n",
    "Con GRU se utilizaron 111 parametros comparados con los 41 que se usaron en la SimpleRNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ziU6QiD3xrHu"
   },
   "outputs": [],
   "source": [
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwBZHop_nVtX"
   },
   "source": [
    "## DOCUMENTS MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P0IWj4Ox5hu"
   },
   "source": [
    "En este modelo utilizaremos como datos de entrenamiento los caracteres y buscaremos predecir el caracter siguiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SShhbX6yb479"
   },
   "source": [
    "Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cURwfPP-b8O8"
   },
   "outputs": [],
   "source": [
    "def vocabulary_obtention(data):\n",
    "  chars = list(data.split())\n",
    "  char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "  int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "  n_chars = len(data)\n",
    "  n_vocab = len(chars)\n",
    "  return char_to_int, int_to_char,n_chars, n_vocab \n",
    "\n",
    "def sequence_arrays(data, seq_length, n_chars):\n",
    "  X = []\n",
    "  Y = []\n",
    "  for i in range(0, n_chars - seq_length):\n",
    "    seq_in = data.split()[i:i + seq_length]\n",
    "    seq_out = data.split()[i + seq_length]\n",
    "    X.append([char_to_int[char] for char in seq_in if char in char_to_int])\n",
    "    Y.append(char_to_int[seq_out]) \n",
    "  n_patterns = len(X)\n",
    "  #n_patterns = 4573338\n",
    "  return X, Y, n_patterns \n",
    "\n",
    "def feature_normalization(X, n_vocab, n_patterns):\n",
    "  X = np.reshape(X, (n_patterns, seq_length, 1))\n",
    "  X = X / float(n_vocab)\n",
    "  return X \n",
    " \n",
    "def save_to_drive(filepath):\n",
    "  # Authenticate and create the PyDrive client.\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "\n",
    "  # Create & upload a file.\n",
    "  uploaded = drive.CreateFile({'title': filepath})\n",
    "  uploaded.SetContentFile(filepath)\n",
    "  uploaded.Upload()\n",
    "  print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
    "\n",
    "def generate_text(X,char_to_int,int_to_char,model, n_vocab, length_text):  \n",
    "  init = np.random.randint(0, len(X)-1)\n",
    "  phrase = X[init]\n",
    "\n",
    "  print (\"Primer parrafo\")\n",
    "  print (\"\\\"\",''.join([int_to_char[value] for value in phrase]), \"\\\"\")\n",
    "\n",
    "  # RNN genera caracteres escogidos  \n",
    "  for i in range(length_text):\n",
    "    x = np.reshape(phrase, (1, len(phrase), 1))\n",
    "    x = x/float(n_vocab)\n",
    "    \n",
    "    # predice un embedding de la palabra \n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    \n",
    "    \n",
    "    # elige al azar uno de los mas altos \n",
    "    index = np.argmax(prediction)\n",
    "    \n",
    "    if index in int_to_char:\n",
    "      # pasar de index a caracter   \n",
    "      result = int_to_char[index]\n",
    "      seq_in = [int_to_char[value] for value in phrase]\n",
    "      sys.stdout.write(result)\n",
    "      phrase.append(index)\n",
    "      phrase = phrase[1:len(phrase)]\n",
    "\n",
    "  print(\"\\nFINISHED.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzPLdFouwjJU"
   },
   "source": [
    "Procesamiento del texto: \n",
    "- creamos un diccionario con cada letra y su index\n",
    "- diccionario con key letra y value el index y vice versa para hacer la busqueda despues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JYiipL5Uiilp"
   },
   "outputs": [],
   "source": [
    "data = open('shakespeare_input.txt', 'r').read()\n",
    "\n",
    "# pasamos todo a minusculas \n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9v09njjdwUlf"
   },
   "outputs": [],
   "source": [
    "char_to_int, int_to_char,n_chars, n_vocab = vocabulary_obtention(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1527696335109,
     "user": {
      "displayName": "Andres Carvallo",
      "photoUrl": "//lh4.googleusercontent.com/-6BjOFE1P4Mk/AAAAAAAAAAI/AAAAAAAAAqI/qXhmQWXy3Y0/s50-c-k-no/photo.jpg",
      "userId": "103607418722343842699"
     },
     "user_tz": 240
    },
    "id": "Qf1HA3_Uy6tR",
    "outputId": "0d1cd9ab-3005-4bfe-dba2-a50810a5a2cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4573338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1527696397795,
     "user": {
      "displayName": "Andres Carvallo",
      "photoUrl": "//lh4.googleusercontent.com/-6BjOFE1P4Mk/AAAAAAAAAAI/AAAAAAAAAqI/qXhmQWXy3Y0/s50-c-k-no/photo.jpg",
      "userId": "103607418722343842699"
     },
     "user_tz": 240
    },
    "id": "MjP4JE_Akh76",
    "outputId": "85ecbb67-9f4a-40f8-fb0a-9e342466a676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total caracteres: 4573338\n",
      "largo vocabulario: 832301\n"
     ]
    }
   ],
   "source": [
    "print('total caracteres: {}'.format(n_chars))\n",
    "print('largo vocabulario: {}'.format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bKstzAPkwVo2"
   },
   "outputs": [],
   "source": [
    "X_array, Y_array, n_patterns = sequence_arrays(data, seq_length = 100, n_chars = n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1527622637145,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "CWUH8W1qr4dW",
    "outputId": "6ad0ee52-604a-4c14-ae91-b06df4139a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patterns: 4573338\n"
     ]
    }
   ],
   "source": [
    "print('total patterns: {}'.format(n_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPbmozzbAIYZ"
   },
   "source": [
    "Normalizamos por el largo del vocabulario y convertimos a one hot encoder el array Y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mCGTM5nW4WQV"
   },
   "outputs": [],
   "source": [
    "# este se cae en colab, hay que subir esta array de dropbox (abajo) \n",
    "X = feature_normalization(X_array, n_vocab, n_patterns)\n",
    "\n",
    "y = np_utils.to_categorical(Y_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0W-iazHBuPhl"
   },
   "source": [
    "Cargamos array X de dropbox (Colab se cae en paso anterior...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32304,
     "status": "ok",
     "timestamp": 1527639852952,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "d53-p47KNR1l",
    "outputId": "2e06d27a-2843-4b5d-c3f3-a550d62a42fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-05-30 00:23:41--  https://www.dropbox.com/s/vn3d90s6n4q7jpo/X.npy\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:6031:1::a27d:5101\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/Fdnf2FpeGqA98CgJEroI4a0RWzS7uAZXsDngmEf8kfLHhxCoOv4DE8tVQkxDlvEU/file [following]\n",
      "--2018-05-30 00:23:42--  https://dl.dropboxusercontent.com/content_link/Fdnf2FpeGqA98CgJEroI4a0RWzS7uAZXsDngmEf8kfLHhxCoOv4DE8tVQkxDlvEU/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3658590528 (3.4G) [application/octet-stream]\n",
      "Saving to: ‘X.npy’\n",
      "\n",
      "X.npy                68%[============>       ]   2.35G   137MB/s    eta 10s    X.npy               100%[===================>]   3.41G   119MB/s    in 29s     \n",
      "\n",
      "2018-05-30 00:24:12 (119 MB/s) - ‘X.npy’ saved [3658590528/3658590528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/vn3d90s6n4q7jpo/X.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1834,
     "status": "ok",
     "timestamp": 1527622731811,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "knAKzowNdZk4",
    "outputId": "277a923b-11a5-4966-c25b-59f3bb3d4880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  shakespeare_input.txt\tX.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EpJkKt5lK1Ih"
   },
   "outputs": [],
   "source": [
    "X = np.load('X.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "recmTCIuAiiv"
   },
   "source": [
    "Es un problema de clasificacion con 41 posibles clases (largo del vocabulario): <br> \n",
    "\n",
    "**LSTM Normal:** <br> \n",
    "- 2 LSTM y un DropOut con prob 0.15 \n",
    "- Incluimos un clipvalue para que el gradiente se mueva dentro del rango -0.7 y 0.7\n",
    "- En la ultima capa una softmax para predecir la siguiente clase \n",
    "- Optimizador SGD\n",
    "- Batchsize de 500\n",
    "\n",
    "\n",
    "**LSTM Bidireccional ** <br> \n",
    "- Capa LSTM Bidireccional \n",
    "- Drop out de 0.15 prob \n",
    "- clipvalue en rango -0.7 y 0.7 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5pTakxTohwX"
   },
   "source": [
    "**AGREGAR DIAGRAMA DE BLOQUES **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tZr4xwUsiJ5N"
   },
   "outputs": [],
   "source": [
    "Image(\"diagrama_LSTM.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SXzL3W7TSHN"
   },
   "source": [
    "**LSTM NORMAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qac645ueTMcQ"
   },
   "outputs": [],
   "source": [
    "#cargar weights \n",
    "#files.upload()\n",
    "\n",
    "# meta-params \n",
    "epochs = 12 \n",
    "batch_size = 500\n",
    "\n",
    "# clipvalue limites de gradiente (-0.7 - 0.7)\n",
    "optimizer = optimizers.Adam(clipvalue = 0.7)\n",
    "\n",
    "# capas del modelo \n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "\n",
    "#model.add(Bidirectional(LSTM(256, return_sequences=True),input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load weights (cambiar dependiendo si queremos iniciar un modelo anterior) \n",
    "#path_weights = 'weights_best_RNN_P2_LSTM_11_epochs.hdf5'\n",
    "#model.load_weights(path_weights)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer )\n",
    "filepath=\"weights_best_RNN_P2_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs= epochs, batch_size= batch_size, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lA16AfRTNEy"
   },
   "source": [
    "**LSTM BIDIRECCIONAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OG9Dul4di-_G"
   },
   "source": [
    "Para LSTM Bidireccional consideramos <br> \n",
    "- 2 capas LSTM Bidireccionales \n",
    "- clipvalue en rango -0.7 y 0.7 \n",
    "- batchsize de 500\n",
    "- drop out con probabilidad de 0.15 despues de cada capa LSTM Bidireccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2229,
     "status": "ok",
     "timestamp": 1527677624473,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "-dq3pwfubcEw",
    "outputId": "a82e3b97-0cbb-475a-8a15-3492df4fea13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  shakespeare_input.txt\tweights_best_RNN_P2_LSTM.hdf5  X.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9B7Eb-mSpTco",
    "outputId": "deee8d89-669a-4878-e411-e2300e806004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "2439000/4573238 [==============>...............] - ETA: 1:19:38 - loss: 2.3473"
     ]
    }
   ],
   "source": [
    "#cargar weights \n",
    "#files.upload()\n",
    "\n",
    "# meta-params \n",
    "epochs = 8 \n",
    "batch_size = 500\n",
    "\n",
    "# clipvalue limites de gradiente (-0.7 - 0.7)\n",
    "optimizer = optimizers.Adam(clipvalue=0.7)\n",
    "\n",
    "# capas del modelo \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True),input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256)))\n",
    "\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load weights (cambiar dependiendo si queremos iniciar un modelo anterior) \n",
    "#path_weights = 'weights_best_RNN_P2_LSTM_1_epochs.hdf5'\n",
    "#model.load_weights(path_weights)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer )\n",
    "filepath=\"weights_best_RNN_P2_LSTM_Bidireccional.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs= epochs, batch_size= batch_size, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1876,
     "status": "ok",
     "timestamp": 1527677294926,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "p4_Imwz6Y1pU",
    "outputId": "915ddaa3-3110-4f21-abff-22051a58769a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file with ID 13FSBNVmEjPPYR5AIrsQDUxYwMs5dDPLb\n"
     ]
    }
   ],
   "source": [
    "#files.upload()\n",
    "weights_file = \"weights_best_RNN_P2_LSTM.hdf5\"\n",
    "\n",
    "# ESTO ES PARA GUARDAR EN DRIVE EL ARCHIVO LLAMADO weights_modelo_final.best.hdf5\n",
    "save_to_drive(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1863,
     "status": "ok",
     "timestamp": 1527677253976,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "mDFGA08WuySJ",
    "outputId": "4ce1b770-fe5a-4d2d-e7e0-eed0756c7bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  shakespeare_input.txt\tweights_best_RNN_P2_LSTM.hdf5  X.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bEgwVJUPcNV"
   },
   "source": [
    "Ahora veremos como nuestra RNN genera texto para LSTM NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 76379,
     "status": "ok",
     "timestamp": 1527468626397,
     "user": {
      "displayName": "Andres Carvallo",
      "photoUrl": "//lh4.googleusercontent.com/-6BjOFE1P4Mk/AAAAAAAAAAI/AAAAAAAAAqI/qXhmQWXy3Y0/s50-c-k-no/photo.jpg",
      "userId": "103607418722343842699"
     },
     "user_tz": 240
    },
    "id": "0v5j3VRQPa7A",
    "outputId": "505523eb-0196-467f-b253-bceccb03fa47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer parrafo\n",
      "\" ld the cutting of my garments would serve the\n",
      "turn, or the breaking of my spanish sword.\n",
      "\n",
      "second lor \"\n",
      "d:\n",
      "why, then the sea were a son to the sea,\n",
      "and therefore i will be a single thought.\n",
      "\n",
      "parolles:\n",
      "a man and the sea will be so so the world,\n",
      "and therefore i will be a state of thee.\n",
      "\n",
      "cassio:\n",
      "why, then the sea were a son of the sea,\n",
      "and therefore the sense is a son of thee.\n",
      "\n",
      "cassio:\n",
      "why, then the sea were a son of the sea,\n",
      "and therefore the sense is a son of thee.\n",
      "\n",
      "cassio:\n",
      "why, then the sea were a son of the sea,\n",
      "and therefore the sense is a son of thee.\n",
      "\n",
      "cassio:\n",
      "why, then the sea were a son of th\n",
      "FINISHED.\n"
     ]
    }
   ],
   "source": [
    "# largo del texto que queremos generar \n",
    "#length_text = 500 \n",
    "\n",
    "#generate_text(X_array, char_to_int, int_to_char, model, n_vocab, length_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3SSq0Qt6Txb5"
   },
   "source": [
    "Vemos que repite muchas veces la frase \"sea were a son of the sea\" esto es porque no se está entrenando con palabras de adelante y de atras como lo haria la LSTM Bidireccional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qL2-vK2-TqnG"
   },
   "source": [
    "Para LSTM BIDERECCIONAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57119,
     "status": "ok",
     "timestamp": 1527677584642,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "09tiJliv2Ptw",
    "outputId": "a7b85e72-a700-4b1c-ec42-379aaab6c29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer parrafo\n",
      "\" ion:\n",
      "no doubt the murderous knife was dull and blunt\n",
      "till it was whetted on thy stone-hard heart,\n",
      "to \"\n",
      " the common of the country of the courtesy.\n",
      "\n",
      "cassio:\n",
      "i will not be a man of the country of the courtesy.\n",
      "\n",
      "cassio:\n",
      "i will not be a man of the country of the court\n",
      "of the court of the court of the court\n",
      "FINISHED.\n"
     ]
    }
   ],
   "source": [
    "# largo del texto que queremos generar \n",
    "length_text = 200 \n",
    "\n",
    "generate_text(X_array, char_to_int, int_to_char, model, n_vocab, length_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-O80GmH-xbkO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "tarea2_deep_learning.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
